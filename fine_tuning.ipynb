{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    formatted_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        instruction = \"Generate domain name suggestions for the following business.\"\n",
    "        input_text = f\"Business description: {row['business_description']}\"\n",
    "        output_text = f\"Suggested domains: {row['suggested_domains']}\"\n",
    "        \n",
    "        formatted_data.append({\n",
    "            \"text\": f\"<s>[INST] {instruction}\\n\\n{input_text} [/INST] {output_text}</s>\"\n",
    "        })\n",
    "    \n",
    "    dataset = Dataset.from_pandas(pd.DataFrame(formatted_data))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples, tokenizer):\n",
    "    max_length = 128\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=max_length, \n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_and_train(dataset, output_dir=\"./finetuned_mistral_domain_generator\"):\n",
    "    train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "    train_dataset = train_test_split[\"train\"]\n",
    "    eval_dataset = train_test_split[\"test\"]\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    train_tokenized = train_dataset.map(\n",
    "        lambda x: tokenize_function(x, tokenizer), \n",
    "        batched=True, \n",
    "        batch_size=8\n",
    "    )\n",
    "    eval_tokenized = eval_dataset.map(\n",
    "        lambda x: tokenize_function(x, tokenizer), \n",
    "        batched=True, \n",
    "        batch_size=8\n",
    "    )\n",
    "    \n",
    "    compute_dtype = getattr(torch, \"float16\")\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True, \n",
    "        llm_int8_enable_fp32_cpu_offload=True \n",
    "    )\n",
    "\n",
    "    max_memory = {0: \"8GiB\", \"cpu\": \"16GiB\"}\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        max_memory=max_memory,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=compute_dtype,\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False \n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        logging_steps=50,\n",
    "        gradient_checkpointing=True,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=50,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        learning_rate=1e-4,\n",
    "        save_steps=400,\n",
    "        fp16=True,\n",
    "        push_to_hub=False,\n",
    "        save_total_limit=1,\n",
    "        ddp_find_unused_parameters=False,\n",
    "        optim=\"adamw_torch\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tokenized,\n",
    "        eval_dataset=eval_tokenized,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_domain_suggestions(model, tokenizer, business_description, max_length=100):\n",
    "    instruction = \"Generate domain name suggestions for the following business.\"\n",
    "    input_text = f\"Business description: {business_description}\"\n",
    "    \n",
    "    prompt = f\"<s>[INST] {instruction}\\n\\n{input_text} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"[/INST]\" in generated_text:\n",
    "        response = generated_text.split(\"[/INST]\")[1].strip()\n",
    "    else:\n",
    "        response = generated_text\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_in_chunks(csv_path, chunk_size=50, output_dir=\"./finetuned_mistral_domain_generator\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    num_chunks = max(1, len(df) // chunk_size)\n",
    "    print(f\"Training in {num_chunks} chunks with {chunk_size} examples per chunk\")\n",
    "    \n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        print(f\"\\n--- Processing chunk {i+1}/{num_chunks} ---\")\n",
    "        \n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, len(df))\n",
    "        chunk_df = df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        temp_csv = f\"temp_chunk_{i}.csv\"\n",
    "        chunk_df.to_csv(temp_csv, index=False)\n",
    "        \n",
    "        dataset = prepare_data(temp_csv)\n",
    "        \n",
    "        model, tokenizer = setup_and_train(dataset, output_dir=f\"{output_dir}_chunk_{i}\")\n",
    "        \n",
    "        if os.path.exists(temp_csv):\n",
    "            os.remove(temp_csv)\n",
    "            \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    if model is not None and tokenizer is not None:\n",
    "        model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using chunked training to minimize memory usage...\n",
      "Training in 2 chunks with 500 examples per chunk\n",
      "\n",
      "--- Processing chunk 1/2 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2344a31458f424d91f88caceda5a784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3e10dbe2f4448197d87afc641ff725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff68f65821854187897f57c5b96e64f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 3407872\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 24:48, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.950343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing chunk 2/2 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e1db3a734241e488a9a68a25cb4b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fba35e12c942aea4ad160ff052cdb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ec2cf4228d461e835ce0e74c63add5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 3407872\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 5:28:51, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.938800</td>\n",
       "      <td>0.936948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning completed!\n",
      "Generated domain suggestions for 'A business that specializes in handmade pottery.':\n",
      "Generate domain name suggestions for the following business.\n",
      "\n",
      "Business description: A business that specializes in handmade pottery.  Suggested domains: handmade.co, handmadepottery.com, handmadepottery.biz, handmadepottery.org, handmadepottery.shop, handmade.shop, handmadepotteryworld.co, handmade.net, handmadepottery.online, handmade.io, handmade.net, handmadepotterycentral.com, handmadepottery.io, handmade\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"data/business_descriptions.csv\"\n",
    "    \n",
    "    use_chunked_training = True\n",
    "    \n",
    "    if use_chunked_training:\n",
    "        print(\"Using chunked training to minimize memory usage...\")\n",
    "        model, tokenizer = train_in_chunks(csv_path, chunk_size=500)\n",
    "    else:\n",
    "        dataset = prepare_data(csv_path)\n",
    "        print(f\"Dataset created with {len(dataset)} examples\")\n",
    "        \n",
    "        print(\"Starting fine-tuning...\")\n",
    "        model, tokenizer = setup_and_train(dataset)\n",
    "    \n",
    "    print(\"Fine-tuning completed!\")\n",
    "    \n",
    "    test_description = \"A business that specializes in handmade pottery.\"\n",
    "    suggestions = generate_domain_suggestions(model, tokenizer, test_description)\n",
    "    print(f\"Generated domain suggestions for '{test_description}':\")\n",
    "    print(suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated domain suggestions for : 'A business that specializes in baking pies.'\n",
      "Generate domain name suggestions for the following business.\n",
      "\n",
      "Business description: A business that specializes in baking pies.  Suggested domains: pie.online, piesbaking.com, pieexpert.online, pie.biz, pie.io, pies.biz, piehub.site, piecentral.online, pie.co, pies.shop, pie.com, piehub.tech, pieexpert.io, piebaking.tech, piebaking.online, piehub.net, pie.co, pie.tech, pies.\n"
     ]
    }
   ],
   "source": [
    "test_description = \"A business that specializes in baking pies.\"\n",
    "suggestions = generate_domain_suggestions(model, tokenizer, test_description, 100)\n",
    "print(f\"Generated domain suggestions for : '{test_description}'\")\n",
    "print(suggestions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
