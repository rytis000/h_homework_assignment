{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "import dns.resolver\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_levenshtein_distance(domain_list):\n",
    "    total_distance = 0\n",
    "    num_pairs = 0\n",
    "    \n",
    "    for i in range(len(domain_list)):\n",
    "        for j in range(i + 1, len(domain_list)):\n",
    "            total_distance += levenshtein_distance(domain_list[i], domain_list[j])\n",
    "            num_pairs += 1\n",
    "    \n",
    "    return total_distance / num_pairs if num_pairs > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_domain_availability(domain):\n",
    "    try:\n",
    "        dns.resolver.resolve(domain, 'A')\n",
    "        return 0\n",
    "    except dns.resolver.NoAnswer:\n",
    "        return 1\n",
    "    except dns.resolver.NXDOMAIN:\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_api(business_desc: str):\n",
    "    print(business_desc)\n",
    "    print(requests.get(f'http://127.0.0.1:8000/generate_domain_name/{business_desc}').json())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'zero_shot': ['sweetpieemporium.com', 'flakyfingersbakery.com', 'piepassionate.com', 'crustcrafters.com', 'pastriesbypastryqueen.com'], \n",
    "                   'few_shot': [\"PieParadise.com\", \"SweetCrust.com\", \"PieMaker.com\", \"PieEmporium.com\", \"PieNirvana.com\"],\n",
    "                   'llm_prompt': [\"PiePerfection.com\", \"SweetPieSensations.com\", \"PieParadise.com\", \"CrispyCrustCreations.com\", \"PiePassionate.com\"],\n",
    "                   'fine_tuned': [\"piebaking.co\", \"piebaking.org\", \"bakingsolutions.biz\", \"piehub.io\", \"piebaking.shop\"]})\n",
    "df = df.reset_index().melt(id_vars=['index']).rename(columns={'variable': 'prompt_type', 'value': 'address'})[['prompt_type', 'address']]\n",
    "df['address'] = df['address'].str.lower()\n",
    "df['domain'] = df['address'].str.split('.').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity_dict = {}\n",
    "distance_dict = {}\n",
    "availability_dict = {}\n",
    "for row in df.groupby('prompt_type')['domain'].apply(lambda x: ' '.join(x)).reset_index().itertuples():\n",
    "    diversity_dict[row.prompt_type] = len(Counter(row.domain.split()).keys()) / len(row.domain.split())\n",
    "for row in df.groupby('prompt_type')['address'].apply(lambda x: list(x)).reset_index().itertuples():\n",
    "    distance_dict[row.prompt_type] = average_levenshtein_distance(row.address)\n",
    "for row in df.groupby('prompt_type')['address'].apply(lambda x: list(x)).reset_index().itertuples():\n",
    "    availability_dict[row.prompt_type] = sum([check_domain_availability(address) for address in row.address]) / len(row.address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [diversity_dict, distance_dict, availability_dict]\n",
    "row_names = ['diversity_score', 'levenshteins_distance', 'availability_score']\n",
    "scores_df = pd.DataFrame(data, index=row_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing API with fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pet grooming\n",
      "['groomingpet.io', 'groomingpetcentral.shop', 'petgrooming.online', 'grooming.com', 'petgroomingzone.tech', 'petgrooming.org']\n",
      "\n",
      "hosting services\n",
      "['hosting.co', 'hostinghub.online', 'hostingsolutions.org', 'hosting.org', 'hostingservicesnow.site', 'hostingsolutions.tech', 'hostingsolutions.com', 'hostingservices.tech']\n",
      "\n",
      "baking pies\n",
      "['piesbaking.site', 'piesbaking.online', 'pies.online', 'piebaking.tech', 'piesbaking.net', 'pie.online', 'pies.co']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "businesses = [\"pet grooming\", \"hosting services\", \"baking pies\"]\n",
    "for business in businesses:\n",
    "    test_api(business)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this experiment is to test, evaluate and implement a feature that suggests domain for a business website based on a user provided business description. To determine best approach, 2 different option were considered: prompt engineering and fine tuning open source LLM. Widely popular Mistral-7B LLM was used for all the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 different tactics were tested for constructing a prompt to get the best results:\n",
    "- Zero-shot. No example was given to the LLM. Prompt - _Suggest 5 creative domain names in csv format for a business with a description of: __business_description__._\n",
    "- Few-shot. Several examples given for LLM to follow. Prompt - _If a business description is 'pet grooming' then their domains could be 'HappyTails.com', 'FurCare.com', 'HappyPaws.com' and similar. If a business description is 'web hosting services' then their domain could be 'hostinger.com'. Suggest 5 unique, short and memorable domain names in csv format for a business with a description of: __business_description__ \"_\n",
    "- Automatic prompt. Asked LLM to propose best prompt for LLM to generate domain names. Prompt - _Generate 5 unique domain names in for a __business_description__. The names should be catchy, easy to remember, and relevant to the __business_description__ industry. provide names in csv format._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained LLM and a small sample of 1000 {business description : domain name} pairs dataset were used to fine tune the model. Quality of data plays a crucial role in successfully fine tuning LLM. While it is also computationally expensive procedure, it can be optimized in several ways, such as splitting training process into chunks reducing required memory for the process (although increasing training time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All 4 different approaches (3 different prompts and fine tuned model) appeared to do reasonably well. To evaluate the results, 3 metrics were chosen: Levenshtein distance, n-gram diversity score and availability score. Levenshtein distance is a string metric for measuring the difference between two sequences. Applied this to all suggestion combinations from one approach we can get an average distance showing how different from each other the suggestions for domain name are. Together with diversity score (unique domain suggestions divided by total suggestions) we can see how much freedom the approach has in generating names. Whether or not this is desirable, we can see that zero-shot approach has the most different suggestions, which makes sense, since we did not provide an example for LLM to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>few_shot</th>\n",
       "      <th>fine_tuned</th>\n",
       "      <th>llm_prompt</th>\n",
       "      <th>zero_shot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>diversity_score</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>levenshteins_distance</th>\n",
       "      <td>7.8</td>\n",
       "      <td>9.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>14.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>availability_score</th>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       few_shot  fine_tuned  llm_prompt  zero_shot\n",
       "diversity_score             1.0         0.6         1.0        1.0\n",
       "levenshteins_distance       7.8         9.5        11.5       14.9\n",
       "availability_score          0.4         1.0         0.6        0.8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Availability score is a percentage of how many suggestions of domains are available and are not taken at this moment. Availability checks indicated that prompt-engineered names had a higher failure rate in terms of domain availability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth noting, Levenshtein distance could also be utilised to filter out the suggestions. For example, we would like to avoid domains that are similar to well known brands, company names and domains. If suggestion has low distance from one of the well known domain, it should be considered to be rulled out. Even though suggestion may not be exact to some known brand, being very close to it makes it seem like a scam website often used for phishing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt engineering is a good option for rapid prototyping and generating domain names quickly. It's flexible and doesn't require a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning with a custom dataset is a powerful technique to improve creativity and relevance, yielding more domain names that are contextually appropriate and unique. Yet high quality data may or may not be available. This together with higher cost of training the model makes this approach best fit for long term, high importance feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good domain name should be short, memorable and do not closely resemble popular and already taken domains. Evaluating each of the suggested name is crucial. Availability is even more important, would be counterproductive to suggest already taken domain. Unfortunately this has to be done after LLM gives the suggestion, no way to train the model to suggest only free domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional information from user, either feedback or tracking, for example, their prefered extensions like .com or .net could be useful to further fine tune the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both fine tuned and base models can be deployed and exposed via API for easy access by others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to try and run API locally:\n",
    "- python -m venv venv\n",
    "- activate venv\n",
    "- pip install -r requirements.txt\n",
    "- uvicorn main:app --reload\n",
    "- http://localhost:8000/docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, fine tuned model is hosted on huggingface spaces (https://huggingface.co/spaces/Rytis-J/domain-generator), but with a free tier, running on limited cpu, it's practically unusable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does work though\n",
    "![alt text](data/hosted_domain_generator.png \"Title\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
